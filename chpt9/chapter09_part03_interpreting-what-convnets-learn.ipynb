{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
    "\n",
    "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
    "\n",
    "This notebook was generated for TensorFlow 2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Interpreting what convnets learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Visualizing intermediate activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# You can use this to load the file \"convnet_from_scratch_with_augmentation.keras\"\n",
    "# you obtained in the last chapter.\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"convnet_from_scratch_with_augmentation.keras\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preprocessing a single image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "img_path = keras.utils.get_file(\n",
    "    fname=\"cat.jpg\",\n",
    "    origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\")\n",
    "\n",
    "def get_img_array(img_path, target_size):\n",
    "    img = keras.utils.load_img(\n",
    "        img_path, target_size=target_size)\n",
    "    array = keras.utils.img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "img_tensor = get_img_array(img_path, target_size=(180, 180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Displaying the test picture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img_tensor[0].astype(\"uint8\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Instantiating a model that returns layer activations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "layer_outputs = []\n",
    "layer_names = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):\n",
    "        layer_outputs.append(layer.output)\n",
    "        layer_names.append(layer.name)\n",
    "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Using the model to compute layer activations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "activations = activation_model.predict(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Visualizing the fifth channel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(first_layer_activation[0, :, :, 5], cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Visualizing every channel in every intermediate activation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "images_per_row = 16\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    n_features = layer_activation.shape[-1]\n",
    "    size = layer_activation.shape[1]\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros(((size + 1) * n_cols - 1,\n",
    "                             images_per_row * (size + 1) - 1))\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_index = col * images_per_row + row\n",
    "            channel_image = layer_activation[0, :, :, channel_index].copy()\n",
    "            if channel_image.sum() != 0:\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n",
    "            display_grid[\n",
    "                col * (size + 1): (col + 1) * size + col,\n",
    "                row * (size + 1) : (row + 1) * size + row] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Visualizing convnet filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Instantiating the Xception convolutional base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-29 12:37:02.217050: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.applications.xception.Xception(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Printing the names of all convolutional layers in Xception**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1\n",
      "block1_conv2\n",
      "block2_sepconv1\n",
      "block2_sepconv2\n",
      "conv2d\n",
      "block3_sepconv1\n",
      "block3_sepconv2\n",
      "conv2d_1\n",
      "block4_sepconv1\n",
      "block4_sepconv2\n",
      "conv2d_2\n",
      "block5_sepconv1\n",
      "block5_sepconv2\n",
      "block5_sepconv3\n",
      "block6_sepconv1\n",
      "block6_sepconv2\n",
      "block6_sepconv3\n",
      "block7_sepconv1\n",
      "block7_sepconv2\n",
      "block7_sepconv3\n",
      "block8_sepconv1\n",
      "block8_sepconv2\n",
      "block8_sepconv3\n",
      "block9_sepconv1\n",
      "block9_sepconv2\n",
      "block9_sepconv3\n",
      "block10_sepconv1\n",
      "block10_sepconv2\n",
      "block10_sepconv3\n",
      "block11_sepconv1\n",
      "block11_sepconv2\n",
      "block11_sepconv3\n",
      "block12_sepconv1\n",
      "block12_sepconv2\n",
      "block12_sepconv3\n",
      "block13_sepconv1\n",
      "block13_sepconv2\n",
      "conv2d_3\n",
      "block14_sepconv1\n",
      "block14_sepconv2\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):\n",
    "        print(layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Creating a feature extractor model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "layer_name = \"block3_sepconv1\"\n",
    "layer = model.get_layer(name=layer_name)\n",
    "feature_extractor = keras.Model(inputs=model.input, outputs=layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Using the feature extractor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "activation = feature_extractor(\n",
    "    keras.applications.xception.preprocess_input(img_tensor)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def compute_loss(image, filter_index):\n",
    "    activation = feature_extractor(image)\n",
    "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
    "    return tf.reduce_mean(filter_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loss maximization via stochastic gradient ascent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_ascent_step(image, filter_index, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        loss = compute_loss(image, filter_index)\n",
    "    grads = tape.gradient(loss, image)\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    image += learning_rate * grads\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Function to generate filter visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "img_width = 200\n",
    "img_height = 200\n",
    "\n",
    "def generate_filter_pattern(filter_index):\n",
    "    iterations = 30\n",
    "    learning_rate = 10.\n",
    "    image = tf.random.uniform(\n",
    "        minval=0.4,\n",
    "        maxval=0.6,\n",
    "        shape=(1, img_width, img_height, 3))\n",
    "    for i in range(iterations):\n",
    "        image = gradient_ascent_step(image, filter_index, learning_rate)\n",
    "    return image[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Utility function to convert a tensor into a valid image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def deprocess_image(image):\n",
    "    image -= image.mean()\n",
    "    image /= image.std()\n",
    "    image *= 64\n",
    "    image += 128\n",
    "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
    "    image = image[25:-25, 25:-25, :]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.axis(\"off\")\n",
    "plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Generating a grid of all filter response patterns in a layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "all_images = []\n",
    "for filter_index in range(64):\n",
    "    print(f\"Processing filter {filter_index}\")\n",
    "    image = deprocess_image(\n",
    "        generate_filter_pattern(filter_index)\n",
    "    )\n",
    "    all_images.append(image)\n",
    "\n",
    "margin = 5\n",
    "n = 8\n",
    "cropped_width = img_width - 25 * 2\n",
    "cropped_height = img_height - 25 * 2\n",
    "width = n * cropped_width + (n - 1) * margin\n",
    "height = n * cropped_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        image = all_images[i * n + j]\n",
    "        stitched_filters[\n",
    "            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n",
    "            (cropped_height + margin) * j : (cropped_height + margin) * j\n",
    "            + cropped_height,\n",
    "            :,\n",
    "        ] = image\n",
    "\n",
    "keras.utils.save_img(\n",
    "    f\"filters_for_layer_{layer_name}.png\", stitched_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Visualizing heatmaps of class activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loading the Xception network with pretrained weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = keras.applications.xception.Xception(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preprocessing an input image for Xception**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://img-datasets.s3.amazonaws.com/elephant.jpg\n",
      "737280/733657 [==============================] - 4s 6us/step\n",
      "745472/733657 [==============================] - 4s 6us/step\n"
     ]
    }
   ],
   "source": [
    "img_path = keras.utils.get_file(\n",
    "    fname=\"elephant.jpg\",\n",
    "    origin=\"https://img-datasets.s3.amazonaws.com/elephant.jpg\")\n",
    "\n",
    "def get_img_array(img_path, target_size):\n",
    "    img = keras.utils.load_img(img_path, target_size=target_size)\n",
    "    array = keras.utils.img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    array = keras.applications.xception.preprocess_input(array)\n",
    "    return array\n",
    "\n",
    "img_array = get_img_array(img_path, target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('n02504458', 'African_elephant', 0.86993986), ('n01871265', 'tusker', 0.076956056), ('n02504013', 'Indian_elephant', 0.023541762)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(img_array)\n",
    "print(keras.applications.xception.decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Setting up a model that returns the last convolutional output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "last_conv_layer_name = \"block14_sepconv2_act\"\n",
    "classifier_layer_names = [\n",
    "    \"avg_pool\",\n",
    "    \"predictions\",\n",
    "]\n",
    "last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Reapplying the classifier on top of the last convolutional output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
    "x = classifier_input\n",
    "for layer_name in classifier_layer_names:\n",
    "    x = model.get_layer(layer_name)(x)\n",
    "classifier_model = keras.Model(classifier_input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Retrieving the gradients of the top predicted class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "    tape.watch(last_conv_layer_output)\n",
    "    preds = classifier_model(last_conv_layer_output)\n",
    "    top_pred_index = tf.argmax(preds[0])\n",
    "    top_class_channel = preds[:, top_pred_index]\n",
    "\n",
    "grads = tape.gradient(top_class_channel, last_conv_layer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Gradient pooling and channel-importance weighting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy()\n",
    "last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "for i in range(pooled_grads.shape[-1]):\n",
    "    last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
    "heatmap = np.mean(last_conv_layer_output, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Heatmap post-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1589a4be0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Superimposing the heatmap on the original picture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "img = keras.utils.load_img(img_path)\n",
    "img = keras.utils.img_to_array(img)\n",
    "\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "jet = cm.get_cmap(\"jet\")\n",
    "jet_colors = jet(np.arange(256))[:, :3]\n",
    "jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
    "jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
    "\n",
    "superimposed_img = jet_heatmap * 0.4 + img\n",
    "superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
    "\n",
    "save_path = \"elephant_cam.jpg\"\n",
    "superimposed_img.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Summary"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter09_part03_interpreting-what-convnets-learn.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
